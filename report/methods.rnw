\newpage
\section{Methods}

\subsection{Data inputs}

The proof of concept contains 3 datasets (vegetation classification, canopy height model, sentinel-2 derived NDVI).

Our initial test region has been anchored North of Lake Tahoe where we accessed a Sentinel-2 imagery for the 9th of May 18:49 (UTC). Within this imagery we downloaded a subset of the 2014 LiDAR dataset from the national center for airborne laser mapping (NCALM) with 5 - 35 cm (8.93 pts/m2) accuracy from a publicly available source (OpenTopography, 2014). The dataset provides pre-calculated digital terrain model (DTM), digital surface model (DSM) and derived canopy height model (CHM). In the future we will best practice workflows to achieve this from the raw LiDAR point clouds. Additionally we incorporated a rasterized version of the fire returnal interval departure (FRID 2019) dataset which contains 34 dominant vegetation classes from the CALVEG. All the datasets are projected into California Albers (EPSG:3310) and raster alignment is verified. We then calculate the normalised difference vegetation index (NVDI) from Sentinel-2 bands (NIR \& Red, 10 m). The 1 m resolution canopy height model is used to train the 10x10 m Sentinel-2 pixels using a linear correlation between the mean canopy height and the NDVI values calculated from the spectral bands. The upscaling process predicts mean canopy height for areas where traditional LiDAR is unavailable.

A detailed documentation for the geospatial processing deployed for our test region can be found on the Github repository\footnote{GitHub repository: \url{https://github.com/Vibrant-Planet/vp-csm}}. Currently GDAL\footnote{GDAL documentation: \url{https://gdal.org/}} is used as the primary source of geospatial processing and all relevant geospatial tools created to convert, reproject and align the datasets are reported. While this initial proof-of-concept is not considered to meet the required level of accuracy, it is designed to demonstrate the overall feasibility of the data collation, exercising all required pathways from data acquisition to output generation.


\subsection{Geospatial Data Abstraction Library (GDAL) Usage}

The Geospatial Data Abstraction Library or GDAL, is an open source library specifically developed for working with raster and vector geospatial data. For raster processing, GDAL libraries are implemented in the backend across the majority of geospatial software tools used today including ArcGIS, FME, and QGIS. The power in GDAL is its command line capability for geospatial data processing. GDAL is is relatively easy to script/scale, is well doucmented, and works with virtually all major geospatial data types.

For the purposes of our project, we are levergaing GDAL 3.2.2 for all our important raster processing:
\begin{itemize}
    \item Reprojections (\texttt{gdalwarp})
    \item Resampling (\texttt{gdal\_translate/gdalwarp})
    \item Masking (\texttt{gdal\_translate/gdalwarp})
    \item Data type conversions (\texttt{gdal\_translate})
    \item Metadata queries (\texttt{gdalinfo})
    \item Raster calculations (\texttt{gdal\_calc.py})
    \item Rasterization (\texttt{gdal\_rasterize})
\end{itemize}

Additionally, we are implementing GDAL's capabilities to create:
\begin{itemize}
    \item Hillshade
    \item Slope
    \item Aspect
    \item Roughness
\end{itemize}

\subsection{PostGIS/PostgreSQL Usage}

PostgreSQL used to manage vector data as tables. PostGIS is an extention for PostgreSQL providing capabilites to manage these tables as geospatial layers.

PostgreSQL > V13.3

PostGIS > V2.4

\subsection{Point Cloud Data Abstraction Library (PDAL) Usage}

PDAL 2.3.0

PDAL is an open library written in C++ for managing and processing point cloud data. This tool is similar to LASTools in the Windows envrionment and offers similar capabilities. The advantages for us in using PDAL for this project are many. One, PDAL is an easily scripted tool allowing for scaling. At its core, PDAL utilizes JSON configurations called pipelines and allows us to string multiple processes into single documents. This method for developing individual complex pipelines for point clouds gives us the control needed to run these processes across myriad cores.  Two, PDAL is developed to handle varied point cloud data types outside of the traditional ASPRS LAS. This may be important as we begin to generate surfaces from varied sources. Three, being an open library, we can if needed build additional capabilities in to the tool. One example being, adding additonal libraries for reading LAS and LAZ formats.

In our project, we are implementing PDAL for point cloud filtering plus, generation of Digital Terrain Models (DTM) and Digital Surface Models (DSM). In a basic PDAL process we will:
\begin{enumerate}
    \item Refine point clouds into ground and non-ground points
    \item Generate Digital Terrain Model (DTM) interpolation from ground points
    \item Generate Digital Surface Model (DSM) interpolation from highest points
\end{enumerate}

Results from the generation of the DTM and DSM are then used to calculate the Canopy Height Model (CHM).

PDAL is not developed specifically for the generation of surface grids (rasters); however, does provide this capability when generating output in geoTIFF format. For the generation of surface rasters, an interpolation method is necessary to create a uniform surface. PDAL integrates an additional library called, points2grid to accomplish this. Points2grid utilizes the Inverse Distance Weighted (IDW) method for its interpolations. This method is powerful in creating accurate interpolations, but can have limitations. Noteably, the IDW method does not interpolate over large areas where data is not present. Specifically, in our surface generations, this results in regions of 'nodata' where there are insufficiant ground points for interpolation.  This is a known outcome and currently addressed by increasing the window size for the IDW to search for neighboring points.  Currently, we are confident in this solution for addressing 'nodata' regions of the surface model; however, we are continuing to research other solutions.

We shoud also note, that the solution to 'nodata' regions in the surface generation can be solved through the creation of Triangulated Irregular Network (TIN) models. TIN models are not an interpolation, but rather a surface generated by connecting points via a triangulated network. Generating TIN models is an excellent method for surface creation of sparsely populated datasets. We have not ruled out the usage of these models; however, due to the point density for our test regions being high, we landed on IDW as our preferred method for the intial surface generation.


\subsection{Canopy Height Model (CHM) Calculation}

Calculation of the CHM is an important aspect of our project and requires a multistep process.
\begin{itemize}
    \item Calculate CHM by: \texttt{DSM - DTM}
\end{itemize}

\subsection{Testing CHM Generation Methods}

\subsection{Additional Refinement for CHM Generation Methods}

\subsection{Sentinel Handling}

\subsection{Alignments}

